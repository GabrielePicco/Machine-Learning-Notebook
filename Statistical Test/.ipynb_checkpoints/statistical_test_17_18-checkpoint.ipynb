{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Statistical tests\n",
    "\n",
    "Apply statistical tests (T-student and Wilcoxon's signed-rank test) on k-nn and decision trees on Iris data with 10 fold cross-validation.\n",
    "For which cases are there statistical differences on the observed accuracies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#perform a t-test for Decision Tree and KNeighborsClassifier\n",
    "def t_test_DT_NN(dataset, n_neighbors = 7, weights = 'distance', min_samples_leaf=1, criteria = 'gini', log=True):\n",
    "    clf_decision_tree = tree.DecisionTreeClassifier(min_samples_leaf = min_samples_leaf, criterion = criteria)\n",
    "    cld_neigh = KNeighborsClassifier(n_neighbors=n_neighbors, weights = weights)\n",
    "\n",
    "    n_split = 10\n",
    "    kf = KFold(n_splits=n_split)\n",
    "    fold = 1\n",
    "    accuracy_difference = []\n",
    "    accuracy_DT = []\n",
    "    accuracy_NN = []\n",
    "    for train, test in kf.split(dataset.data,dataset.target):\n",
    "        clf_decision_tree.fit(dataset.data[train], dataset.target[train])\n",
    "        accuracy_decision_tree = accuracy_score(dataset.target[test], clf_decision_tree.predict(dataset.data[test]))\n",
    "        cld_neigh.fit(dataset.data[train], dataset.target[train])\n",
    "        accuracy_neigh = accuracy_score(dataset.target[test], cld_neigh.predict(dataset.data[test]))\n",
    "        if log : \n",
    "            print \"\\nFold: %s - Accuracy: (Decision Tree Classifier): %s\" % (fold , accuracy_decision_tree)\n",
    "            print \"Fold: %s - Accuracy: (Decision KNeighborsClassifier): %s\" % (fold , accuracy_neigh)\n",
    "            print \"DT-NN: %s\" % (accuracy_decision_tree-accuracy_neigh)\n",
    "        accuracy_difference.append(accuracy_decision_tree-accuracy_neigh)\n",
    "        accuracy_DT.append(accuracy_decision_tree)\n",
    "        accuracy_NN.append(accuracy_neigh)\n",
    "        fold += 1\n",
    "    if log:\n",
    "        print \"\\nAVG: %s\" % np.mean(accuracy_difference)\n",
    "        print \"\\nStdev: %s\" % np.std(accuracy_difference)\n",
    "\n",
    "    #Null hypothesis rejected if the p-value is smaller than the significance level (Es: a = 0.05)\n",
    "    statistic,p_value = stats.ttest_rel(accuracy_DT, accuracy_NN)\n",
    "    print \"\\np-value: %s < 0.05 == %s\" % (p_value, str(p_value < 0.05))\n",
    "    print \"\\nNull hypothesis {0} rejected, the difference between the two algorithms is {0} found significant.\".format(\"\" if p_value < 0.05 else \"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold: 1 - Accuracy: (Decision Tree Classifier): 1.0\n",
      "Fold: 1 - Accuracy: (Decision KNeighborsClassifier): 1.0\n",
      "DT-NN: 0.0\n",
      "\n",
      "Fold: 2 - Accuracy: (Decision Tree Classifier): 1.0\n",
      "Fold: 2 - Accuracy: (Decision KNeighborsClassifier): 1.0\n",
      "DT-NN: 0.0\n",
      "\n",
      "Fold: 3 - Accuracy: (Decision Tree Classifier): 1.0\n",
      "Fold: 3 - Accuracy: (Decision KNeighborsClassifier): 1.0\n",
      "DT-NN: 0.0\n",
      "\n",
      "Fold: 4 - Accuracy: (Decision Tree Classifier): 0.933333333333\n",
      "Fold: 4 - Accuracy: (Decision KNeighborsClassifier): 1.0\n",
      "DT-NN: -0.0666666666667\n",
      "\n",
      "Fold: 5 - Accuracy: (Decision Tree Classifier): 0.933333333333\n",
      "Fold: 5 - Accuracy: (Decision KNeighborsClassifier): 0.866666666667\n",
      "DT-NN: 0.0666666666667\n",
      "\n",
      "Fold: 6 - Accuracy: (Decision Tree Classifier): 0.866666666667\n",
      "Fold: 6 - Accuracy: (Decision KNeighborsClassifier): 0.866666666667\n",
      "DT-NN: 0.0\n",
      "\n",
      "Fold: 7 - Accuracy: (Decision Tree Classifier): 1.0\n",
      "Fold: 7 - Accuracy: (Decision KNeighborsClassifier): 1.0\n",
      "DT-NN: 0.0\n",
      "\n",
      "Fold: 8 - Accuracy: (Decision Tree Classifier): 0.866666666667\n",
      "Fold: 8 - Accuracy: (Decision KNeighborsClassifier): 0.933333333333\n",
      "DT-NN: -0.0666666666667\n",
      "\n",
      "Fold: 9 - Accuracy: (Decision Tree Classifier): 0.933333333333\n",
      "Fold: 9 - Accuracy: (Decision KNeighborsClassifier): 0.866666666667\n",
      "DT-NN: 0.0666666666667\n",
      "\n",
      "Fold: 10 - Accuracy: (Decision Tree Classifier): 1.0\n",
      "Fold: 10 - Accuracy: (Decision KNeighborsClassifier): 1.0\n",
      "DT-NN: 0.0\n",
      "\n",
      "AVG: 0.0\n",
      "\n",
      "Stdev: 0.0421637021356\n",
      "\n",
      "p-value: 1.0 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "t_test_DT_NN(iris, n_neighbors = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For K = 1\n",
      "\n",
      "p-value: 0.343436396138 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 2\n",
      "\n",
      "p-value: 0.212579892323 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 3\n",
      "\n",
      "p-value: 0.678309741806 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 4\n",
      "\n",
      "p-value: 0.555445442106 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 5\n",
      "\n",
      "p-value: 0.678309741806 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 6\n",
      "\n",
      "p-value: 1.0 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 7\n",
      "\n",
      "p-value: 0.343436396138 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 8\n",
      "\n",
      "p-value: 0.678309741806 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 9\n",
      "\n",
      "p-value: 1.0 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 10\n",
      "\n",
      "p-value: 0.44333185017 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n"
     ]
    }
   ],
   "source": [
    "#Different valuse of K (1-10)\n",
    "for k in range (1,11):\n",
    "    print \"\\n\\nFor K = %s\" % k\n",
    "    t_test_DT_NN(iris, n_neighbors = k, log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def get_ramdom_params():\n",
    "    rnd = random.randint(0, 1)\n",
    "    if rnd == 1:\n",
    "        criteria = 'gini'\n",
    "    else:\n",
    "        criteria = 'entropy'\n",
    "    rnd = random.randint(0, 1)\n",
    "    if rnd == 1:\n",
    "        weights = 'uniform'\n",
    "    else:\n",
    "        weights = 'distance'\n",
    "    k = random.randint(1, 20)\n",
    "    min_samples_leaf = random.randint(1, 20)\n",
    "    return k,weights,min_samples_leaf,criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For K = 7 | Weights: distance | min_samples_leaf = 10 | criteria = gini\n",
      "\n",
      "p-value: 0.193422059603 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 12 | Weights: uniform | min_samples_leaf = 10 | criteria = entropy\n",
      "\n",
      "p-value: 1.0 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 13 | Weights: uniform | min_samples_leaf = 16 | criteria = gini\n",
      "\n",
      "p-value: 0.678309741806 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 7 | Weights: distance | min_samples_leaf = 12 | criteria = entropy\n",
      "\n",
      "p-value: 0.193422059603 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 5 | Weights: uniform | min_samples_leaf = 1 | criteria = gini\n",
      "\n",
      "p-value: 0.678309741806 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 4 | Weights: uniform | min_samples_leaf = 9 | criteria = gini\n",
      "\n",
      "p-value: 0.591051231784 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 4 | Weights: distance | min_samples_leaf = 20 | criteria = gini\n",
      "\n",
      "p-value: 0.278872999249 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 11 | Weights: distance | min_samples_leaf = 9 | criteria = gini\n",
      "\n",
      "p-value: 0.193422059603 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 6 | Weights: uniform | min_samples_leaf = 14 | criteria = gini\n",
      "\n",
      "p-value: 0.591051231784 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 19 | Weights: uniform | min_samples_leaf = 2 | criteria = gini\n",
      "\n",
      "p-value: 1.0 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n"
     ]
    }
   ],
   "source": [
    "#t-test with random k value, random distance \"wighted\" or uniform, random number of samples per leaf and random split\n",
    "#criteria\n",
    "for i in range (1,11):\n",
    "    k,weights,min_samples_leaf,criteria = get_ramdom_params()\n",
    "    print \"\\n\\nFor K = %s | Weights: %s | min_samples_leaf = %s | criteria = %s\" % (k,weights,min_samples_leaf,criteria)\n",
    "    t_test_DT_NN(iris, n_neighbors = k, weights = weights, min_samples_leaf = min_samples_leaf, criteria = criteria, log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform a Wilcoxon's for Decision Tree and KNeighborsClassifier\n",
    "def wilcoxon_test_DT_NN(dataset, n_neighbors = 7, weights = 'weights', min_samples_leaf=1, criteria = 'gini', log=True):\n",
    "    clf_decision_tree = tree.DecisionTreeClassifier(min_samples_leaf = min_samples_leaf, criterion = criteria)\n",
    "    cld_neigh = KNeighborsClassifier(n_neighbors=n_neighbors, weights = weights)\n",
    "\n",
    "    n_split = 10\n",
    "    kf = KFold(n_splits=n_split)\n",
    "    fold = 1\n",
    "    accuracy_difference = []\n",
    "    accuracy_DT = []\n",
    "    accuracy_NN = []\n",
    "    for train, test in kf.split(dataset.data,dataset.target):\n",
    "        clf_decision_tree.fit(dataset.data[train], dataset.target[train])\n",
    "        accuracy_decision_tree = accuracy_score(dataset.target[test], clf_decision_tree.predict(dataset.data[test]))\n",
    "        cld_neigh.fit(dataset.data[train], dataset.target[train])\n",
    "        accuracy_neigh = accuracy_score(dataset.target[test], cld_neigh.predict(dataset.data[test]))\n",
    "        if log : \n",
    "            print \"\\nFold: %s - Accuracy: (Decision Tree Classifier): %s\" % (fold , accuracy_decision_tree)\n",
    "            print \"Fold: %s - Accuracy: (Decision KNeighborsClassifier): %s\" % (fold , accuracy_neigh)\n",
    "            print \"DT-NN: %s\" % (accuracy_decision_tree-accuracy_neigh)\n",
    "        accuracy_difference.append(accuracy_decision_tree-accuracy_neigh)\n",
    "        accuracy_DT.append(accuracy_decision_tree)\n",
    "        accuracy_NN.append(accuracy_neigh)\n",
    "        fold += 1\n",
    "    if log:\n",
    "        print \"\\nAVG: %s\" % np.mean(accuracy_difference)\n",
    "        print \"\\nStdev: %s\" % np.std(accuracy_difference)\n",
    "\n",
    "    #Null hypothesis rejected if the p-value is smaller than the significance level (Es: a = 0.05)\n",
    "    statistic,p_value = stats.wilcoxon(accuracy_DT, accuracy_NN)\n",
    "    print \"\\np-value: %s < 0.05 == %s\" % (p_value, str(p_value < 0.05))\n",
    "    print \"\\nNull hypothesis {0} rejected, the difference between the two algorithms is {0} found significant.\".format(\"\" if p_value < 0.05 else \"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "For K = 7 | Weights: uniform | min_samples_leaf = 10 | criteria = gini\n",
      "\n",
      "p-value: 1.0 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 13 | Weights: uniform | min_samples_leaf = 20 | criteria = entropy\n",
      "\n",
      "p-value: 0.654720846019 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 16 | Weights: distance | min_samples_leaf = 18 | criteria = gini\n",
      "\n",
      "p-value: 0.563702861651 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 3 | Weights: distance | min_samples_leaf = 5 | criteria = entropy\n",
      "\n",
      "p-value: 0.654720846019 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 4 | Weights: distance | min_samples_leaf = 14 | criteria = entropy\n",
      "\n",
      "p-value: 0.256839257958 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 1 | Weights: distance | min_samples_leaf = 4 | criteria = entropy\n",
      "\n",
      "p-value: 0.193646431269 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 17 | Weights: uniform | min_samples_leaf = 20 | criteria = gini\n",
      "\n",
      "p-value: 0.479500122187 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 8 | Weights: uniform | min_samples_leaf = 9 | criteria = gini\n",
      "\n",
      "p-value: 1.0 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 15 | Weights: distance | min_samples_leaf = 20 | criteria = gini\n",
      "\n",
      "p-value: 0.563702861651 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n",
      "\n",
      "\n",
      "For K = 6 | Weights: distance | min_samples_leaf = 16 | criteria = gini\n",
      "\n",
      "p-value: 0.256839257958 < 0.05 == False\n",
      "\n",
      "Null hypothesis not rejected, the difference between the two algorithms is not found significant.\n"
     ]
    }
   ],
   "source": [
    "#Wilcoxon's test with random k value, random distance \"wighted\" or uniform, random number of samples per leaf and random split\n",
    "#criteria\n",
    "for i in range (1,11):\n",
    "    k,weights,min_samples_leaf,criteria = get_ramdom_params()\n",
    "    print \"\\n\\nFor K = %s | Weights: %s | min_samples_leaf = %s | criteria = %s\" % (k,weights,min_samples_leaf,criteria)\n",
    "    wilcoxon_test_DT_NN(iris, n_neighbors = k, weights = weights, min_samples_leaf = min_samples_leaf, criteria = criteria, log = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
