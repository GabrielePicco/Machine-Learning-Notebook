{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Decomposition\n",
    "\n",
    "In this excercise we will experiment with the bias-variance decomposition we introduced in the first part of the course.\n",
    "\n",
    "To do that, we will need to evaluate the average error a **single learning algorithm** commits on a **fixed example** when trained over different subsamples of a given dataset. To make things more interesting we will evaluate the average error over several examples and average  the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We will make use of a small synthetic dataset $D$.  Examples are pairs $(x,y) \\in \\mathbb{R}^2$ and the dataset is  a numpy array such that `D[i,:]` contains the $i^\\text{th}$ example (consequently the array slice `D[:,0]`  contains all $x$s, while `D[:,1]` contains all the $y$s).\n",
    "\n",
    "The dataset has been split into two parts, a training set named `Dtrain` and a test set named `Dtest`.\n",
    "\n",
    "Let us begin by importing the training set and the test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bias_var_data import Dtrain, Dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above we will need to train a number of regressors over sub-samples of the training set. The following function will return a new dataset of the given `size` by subsampling from the given dataset `D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subsample(D,size):\n",
    "    Ds = D[random.random_integers(0,len(D)-1,size), :]\n",
    "    return (Ds[:,0, np.newaxis], Ds[:,1, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a regressor for one of the datasets, we shall fit a polynomial to it. The following function will fit a polynomial of the specified `degree` to the given dataset (here the dataset needs to be specified as a pair of `np.arrays`. `X` needs to contain the descriptions of the examples, `Y` needs to contain the corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_regressor(X,Y,degree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X, Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the performances of the learning algorithm implemented in `build_regressor` as you vary the degree of the fitted polynomial from 0 to 9. Build 100 subsamples of `Dtrain` and perform the bias-variance error decomposition over the examples in `Dtest`. To best explain the details of the work you need to perform, the description of the work has been divided into three parts:\n",
    "\n",
    "## First part\n",
    "\n",
    "- Create 100 subsamples of size `15` of `Dtrain`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabriele/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: DeprecationWarning: This function is deprecated. Please call randint(0, 99 + 1) instead\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "subsamples = []\n",
    "for i in range(100):\n",
    "    subsamples.append(subsample(Dtrain, 15))\n",
    "subsamples = np.array(subsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for each degree `d` in $[0..9]$ and for each subsample `Dsub`:\n",
    "    - fit a model of the degree `d` over the `Dsub`;\n",
    "    - use the `.predict` method of the built model to make predictions for the examples in `Dtest`;\n",
    "- collect the resulting data into a `np.array` and call it `results`;\n",
    "    - if you do everything properly, the attribute `.shape` of `results` should return the tuple (10, 100, 30, 1), meaning that `results` should have:\n",
    "        - one entry for each one of the 10 models;\n",
    "        - each \"model\" entry, should have one entry for each one of the 100 subsamples;\n",
    "        - each \"subsample\" entry should have one entry for each one of the 30 examples;\n",
    "        - each \"example\" entry should have a single real value containing the prediction from the `.predict` method.\n",
    "            \n",
    "    - put in simpler terms, `result` should be such that `results[m,s,e,0]` returns the prediction of model `m` over example `e` of subsample `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "degrees = list(range(10))\n",
    "for d in degrees:\n",
    "    preds = []\n",
    "    for sub in subsamples:\n",
    "        model = build_regressor(sub[0],sub[1],d)\n",
    "        pred = model.predict(Dtest[:,0].reshape(-1, 1))\n",
    "        preds.append(pred)\n",
    "    results.append(preds)\n",
    "results = np.array(results)\n",
    "print results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part\n",
    "\n",
    "Complete the following functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `avg_error(` $\\textbf{y}'$ `,` $y$ `)`: given a vector of predictions $\\textbf{y}'$ and the correct prediction $y$ returns the average squared error committed by predictions in $\\textbf{y}'$ w.r.t. $y$;\n",
    "- `bias(` $\\textbf{y}'$ `,` $y$ `)`: given the same parameters of `avg_error` returns the bias term of the squared error;\n",
    "- `variance(` $\\textbf{y}'$ `)`: given a vector of predictions $\\textbf{y}'$ returns the variance term of the squared error;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avg_error(y_, y):\n",
    "    l = [(a-b)**2 for a,b in zip(y_, itertools.repeat(y))]\n",
    "    return np.mean(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bias(y_, y):\n",
    "    e_y_ = np.mean(y_)\n",
    "    return (y-e_y_)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def variance(y_):\n",
    "    e_y_ = np.mean(y_)\n",
    "    l = [(a-b)**2 for a,b in zip(y_, itertools.repeat(e_y_))]\n",
    "    return np.mean(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions aggregate the values you computed (they will be useful in the final part of the exercise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Builds an array containing, for each model and each example, the bias term, the variance \n",
    "# term and the average error .\n",
    "\n",
    "def BVEs(ys_, ys):\n",
    "    return np.array([[bias( ys_[:,ex], ys[ex] ), \n",
    "                      variance( ys_[:,ex] ), \n",
    "                      avg_error( ys_[:,ex], ys[ex])] for ex in range(len(ys))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computes the average over all examples of:\n",
    "#    - the model's average squared error `e`\n",
    "#    - the bias term `b` of the model's error\n",
    "#    - the variance term `v` of the model's error\n",
    "# and returns them as the tuple (b,v,e) \n",
    "def model_avg_bves(data, ys, model):\n",
    "    bves = BVEs(data[model,:,:], ys)\n",
    "    return [np.average(bves[:,0]), np.average(bves[:,1]), np.average(bves[:,2])]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third part\n",
    "\n",
    "Let us now put in fruition our work by displaying, for each model, the three components of the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model degree\t                Bias\t            Variance\t               Error\n",
      "------------\t--------------------\t--------------------\t--------------------\n",
      "           0\t               54.44\t                4.58\t               59.02\n",
      "           1\t               53.50\t               15.19\t               68.69\n",
      "           2\t                6.40\t                1.93\t                8.33\n",
      "           3\t                6.39\t                4.16\t               10.55\n",
      "           4\t                6.11\t               17.67\t               23.78\n",
      "           5\t                9.70\t              214.19\t              223.89\n",
      "           6\t               21.20\t             7813.64\t             7834.84\n",
      "           7\t              384.95\t           234962.13\t           235347.07\n",
      "           8\t           239055.02\t         12130877.43\t         12369932.45\n",
      "           9\t           125009.13\t        211730378.26\t        211855387.39\n"
     ]
    }
   ],
   "source": [
    "models_bves = np.array([model_avg_bves(results, Dtest[:,1], m) for m in range(len(degrees))])\n",
    "\n",
    "print(\"%12s\\t%20s\\t%20s\\t%20s\") % (\"Model degree\", \"Bias\",\"Variance\",\"Error\")\n",
    "print(\"%12s\\t%20s\\t%20s\\t%20s\") % (\"-\"*12,\"-\"*20,\"-\"*20,\"-\"*20)\n",
    "\n",
    "for i,(b,v,e) in enumerate(models_bves):\n",
    "    print(\"%12d\\t%20.2f\\t%20.2f\\t%20.2f\") % (i,b,v,e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that indeed the sum of the bias and the variance terms equals the error\n",
    "\n",
    "Assume you are devising a new system and that trying a new model is costly. You cannot affor to explore a large number of models. Describe how would you proceed to explore the space of the possible models basing your decisions on the bias/variance decomposition. You can use the above data as a running example.\n",
    "\n",
    "I could increase the degree of polynomial until the error decreases."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
